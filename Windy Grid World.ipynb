{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 RL.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omQbk7vcqQQ9"
      },
      "source": [
        "# Assignment 2 RL\n",
        "### \"Windy GridWorld Game\"\n",
        "\n",
        "\n",
        "Mina Rahmanian Shahri (20137470)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_R9tq-s1G92a"
      },
      "source": [
        "import numpy as np\n",
        "np.seterr(over='ignore')\n",
        "import random\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from __future__ import print_function\n",
        "from python_utils.import_ import import_global"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V67K7LDKMC96"
      },
      "source": [
        "## 4 Action Windy Gridworld Shown\n",
        "\n",
        "Implementation of several algorithms (including: Sarsa, Q-learning, Sarsa(ùúÜ),and Watkins‚Äôs Q(ùúÜ) )  to solve this game's problem by 4 action (Up, Down, Right and Left).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roZg7OyaHd3z"
      },
      "source": [
        "\n",
        "# wind gridworld height\n",
        "WORLD_HEIGHT = 7\n",
        "\n",
        "# wind gridworld width\n",
        "WORLD_WIDTH = 10\n",
        "\n",
        "# probability for exploration\n",
        "EPSILON = 0.01\n",
        "\n",
        "# Learning rate or step size\n",
        "ALPHA = 0.1\n",
        "\n",
        "# reward\n",
        "REWARD = -1.0\n",
        "\n",
        "# gamma\n",
        "GAMMA = 0.9\n",
        "\n",
        "# lambda\n",
        "LAMBDA = 0.8\n",
        "\n",
        "# wind strength for each column respectively\n",
        "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "\n",
        "# possible 4 actions\n",
        "ACTION_UP = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_LEFT = 2\n",
        "ACTION_RIGHT = 3\n",
        "\n",
        "# value of the pairs state action\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "START = [3, 0]\n",
        "GOAL = [3, 7]\n",
        "\n",
        "# eligibility trace\n",
        "e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "\n",
        "###############################  Functions  ####################################    \n",
        "\n",
        "# state transitions\n",
        "def step(currentState, currentAction):\n",
        "\n",
        "    i, j = currentState\n",
        "    \n",
        "    # Up\n",
        "    if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - WIND[j], 0), j]\n",
        "    # Down\n",
        "    elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
        "    # Left\n",
        "    elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
        "    # Right\n",
        "    elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "# interaction in a Sarsa episode\n",
        "def oneEpisode_s(Q):\n",
        "  \n",
        "    # track the total time steps in this episode\n",
        "    time = 0\n",
        "\n",
        "    # initialize state\n",
        "    currentState = START\n",
        "\n",
        "    # choose an action based on epsilon-greedy algorithm\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:   \n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "\n",
        "        # Sarsa update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "                            \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "# interaction in a Sarsa_Lambda episode\n",
        "def oneEpisode_S_lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    currentState = START\n",
        "\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Sarsa Lambda update\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], newAction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    e[i ,j, action] *= LAMBDA * GAMMA\n",
        "\n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Learning episode\n",
        "def oneEpisode_q(Q):\n",
        "    \n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "    \n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        # selecting the new action among all possible action which gives the MAX Q\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Q-learning update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "\n",
        "        currentState = newState\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Lambda episode\n",
        "def oneEpisode_q_Lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Watkins‚Äôs Q_Lambda update          \n",
        "        Bestaction = np.argmax(Q[newState[0], newState[1], actions])\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], Bestaction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    if newAction == Bestaction:\n",
        "                          e[i ,j, action] *= LAMBDA * GAMMA\n",
        "                    else:\n",
        "                          e[i ,j, action] == 0      \n",
        "\n",
        "  \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# Preparing functions for plots\n",
        "\n",
        "def sarsa():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_s(Q))\n",
        "            time = oneEpisode_s(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='royalblue')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 4 action SARSA, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "\n",
        "\n",
        "    \n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\t    Optimal policy SARSA is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "\n",
        "def q_learning():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q(Q))\n",
        "            time = oneEpisode_q(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='forestgreen')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 4 action Q-learning, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\t Optimal policy Q-learning is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "def sarsa_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_S_lambda(Q))\n",
        "            time = oneEpisode_S_lambda(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='red')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 4 action SARSA ($\\lambda$), alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\tOptimal policy SARSA_lambda is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column \\n')\n",
        "\n",
        "\n",
        "def q_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q_Lambda(Q))\n",
        "            time = oneEpisode_q_Lambda(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='purple',)\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 4 action Watkins‚Äôs Q ($\\lambda$), alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\tOptimal policy Watkins‚Äôs Q(Œª) is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    sarsa()\n",
        "    q_learning()\n",
        "    sarsa_Lambda()\n",
        "    q_Lambda()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-8t3ZFcZNYT"
      },
      "source": [
        "## Compare & Convergence --> prepare for figure\n",
        "\n",
        "The convergence of the original implementation is compared with the implementation of all algorithms using a graph of Average Accumulated Steps vs. Episode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1hsnNfBZpvL"
      },
      "source": [
        "\n",
        "#######################   Convergence   ######################\n",
        "\n",
        "\n",
        "WORLD_HEIGHT = 7\n",
        "WORLD_WIDTH = 10\n",
        "\n",
        "EPSILON = 0.01\n",
        "ALPHA = 0.1\n",
        "REWARD = -1.0\n",
        "GAMMA = 0.9\n",
        "LAMBDA = 0.8\n",
        "\n",
        "# wind \n",
        "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "\n",
        "ACTION_UP = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_LEFT = 2\n",
        "ACTION_RIGHT = 3\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "START = [3, 0]\n",
        "GOAL = [3, 7]\n",
        "\n",
        "# eligibility trace\n",
        "e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "###############################  Functions  ####################################    \n",
        "\n",
        "# state transitions\n",
        "def step(currentState, currentAction):\n",
        "\n",
        "    i, j = currentState\n",
        "    \n",
        "    # Up\n",
        "    if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - WIND[j], 0), j]\n",
        "    # Down\n",
        "    elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
        "    # Left\n",
        "    elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
        "    # Right\n",
        "    elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "# interaction in a Sarsa episode\n",
        "def oneEpisode_s(Q):\n",
        "  \n",
        "    # track the total time steps in this episode\n",
        "    time = 0\n",
        "\n",
        "    # initialize state\n",
        "    currentState = START\n",
        "\n",
        "    # choose an action based on epsilon-greedy algorithm\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:   \n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "\n",
        "        # Sarsa update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "                            \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "# interaction in a Sarsa_Lambda episode\n",
        "def oneEpisode_S_lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    currentState = START\n",
        "\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Sarsa Lambda update\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], newAction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    e[i ,j, action] *= LAMBDA * GAMMA\n",
        "\n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Learning episode\n",
        "def oneEpisode_q(Q):\n",
        "    \n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "    \n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        # selecting the new action among all possible action which gives the MAX Q\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Q-learning update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "\n",
        "        currentState = newState\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Lambda episode\n",
        "def oneEpisode_q_Lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Watkins‚Äôs Q_Lambda update          \n",
        "        Bestaction = np.argmax(Q[newState[0], newState[1], actions])\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], Bestaction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    if newAction == Bestaction:\n",
        "                          e[i ,j, action] *= LAMBDA * GAMMA\n",
        "                    else:\n",
        "                          e[i ,j, action] == 0      \n",
        "\n",
        "  \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preparing functions for plots\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "total_times = 0\n",
        "x_s =[]\n",
        "y_s =[]\n",
        "while ep < episodeLimit:\n",
        "\n",
        "          total_times += oneEpisode_s(Q)\n",
        "          x_s.append(ep)\n",
        "          y_s.append(total_times/(ep+1))\n",
        "          ep += 1\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "total_times = 0\n",
        "x_q=[]\n",
        "y_q=[]\n",
        "while ep < episodeLimit:\n",
        "\n",
        "          total_times += oneEpisode_q(Q)\n",
        "          x_q.append(ep)\n",
        "          y_q.append(total_times/(ep+1))\n",
        "          ep += 1\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "total_times = 0\n",
        "x_sl=[]\n",
        "y_sl=[]\n",
        "while ep < episodeLimit:\n",
        "\n",
        "          total_times += oneEpisode_S_lambda(Q)\n",
        "          x_sl.append(ep)\n",
        "          y_sl.append(total_times/(ep+1))\n",
        "          ep += 1\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "total_times = 0\n",
        "x_ql=[]\n",
        "y_ql=[]\n",
        "while ep < episodeLimit:\n",
        "\n",
        "          total_times += oneEpisode_q_Lambda(Q)\n",
        "          x_ql.append(ep)\n",
        "          y_ql.append(total_times/(ep+1))\n",
        "          ep += 1\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.plot(x_s, y_s, color='darkblue', label='Sarsa')\n",
        "plt.plot(x_q, y_q, color='seagreen', label='Q-Learning')\n",
        "plt.plot(x_sl, y_sl, color='red', label='Sarsa(lambda)')\n",
        "plt.plot(x_ql, y_ql, color='gold', label='Q(lambda)')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Average time steps')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wWBVbwMPgVQ"
      },
      "source": [
        "# Compare all algorithms in 4 action\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeJux4eTVrD6"
      },
      "source": [
        "\n",
        "#######################   Compare   ######################\n",
        "\n",
        "WORLD_HEIGHT = 7\n",
        "WORLD_WIDTH = 10\n",
        "EPSILON = 0.01\n",
        "ALPHA = 0.1\n",
        "REWARD = -1.0\n",
        "GAMMA = 0.9\n",
        "LAMBDA = 0.8\n",
        "\n",
        "# wind \n",
        "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "\n",
        "ACTION_UP = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_LEFT = 2\n",
        "ACTION_RIGHT = 3\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "START = [3, 0]\n",
        "GOAL = [3, 7]\n",
        "\n",
        "# eligibility trace\n",
        "e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "\n",
        "###############################  Functions  ####################################    \n",
        "\n",
        "# state transitions\n",
        "def step(currentState, currentAction):\n",
        "\n",
        "    i, j = currentState\n",
        "    \n",
        "    # Up\n",
        "    if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - WIND[j], 0), j]\n",
        "    # Down\n",
        "    elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
        "    # Left\n",
        "    elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
        "    # Right\n",
        "    elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "# interaction in a Sarsa episode\n",
        "def oneEpisode_s(Q):\n",
        "  \n",
        "    # track the total time steps in this episode\n",
        "    time = 0\n",
        "\n",
        "    # initialize state\n",
        "    currentState = START\n",
        "\n",
        "    # choose an action based on epsilon-greedy algorithm\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:   \n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "\n",
        "        # Sarsa update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "                            \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "# interaction in a Sarsa_Lambda episode\n",
        "def oneEpisode_S_lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    currentState = START\n",
        "\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Sarsa Lambda update\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], newAction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    e[i ,j, action] *= LAMBDA * GAMMA\n",
        "\n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Learning episode\n",
        "def oneEpisode_q(Q):\n",
        "    \n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "    \n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        # selecting the new action among all possible action which gives the MAX Q\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Q-learning update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "\n",
        "        currentState = newState\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-Lambda episode\n",
        "def oneEpisode_q_Lambda(Q):\n",
        "\n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Watkins‚Äôs Q_Lambda update          \n",
        "        Bestaction = np.argmax(Q[newState[0], newState[1], actions])\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], Bestaction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    if newAction == Bestaction:\n",
        "                          e[i ,j, action] *= LAMBDA * GAMMA\n",
        "                    else:\n",
        "                          e[i ,j, action] == 0      \n",
        "\n",
        "  \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes1 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_s(Q))\n",
        "            time = oneEpisode_s(Q)\n",
        "            episodes1.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes2 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q(Q))\n",
        "            time = oneEpisode_q(Q)\n",
        "            episodes2.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes3 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_S_lambda(Q))\n",
        "            time = oneEpisode_S_lambda(Q)\n",
        "            episodes3.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes4 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q_Lambda(Q))\n",
        "            time = oneEpisode_q_Lambda(Q)\n",
        "            episodes4.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(episodes1, color='darkblue',label=\"SARSA\")\n",
        "plt.plot(episodes3, color='seagreen',label=\"SARSA(Œª)\")\n",
        "plt.plot(episodes2, color='red',label=\"Q Learning\")\n",
        "plt.plot(episodes4, color='gold',label=\"Q(Œª)\")\n",
        "plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "plt.title(\"All (Windy Gridworld) algorithms - 4 action, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('Time steps')\n",
        "plt.ylabel('Episodes')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2CMZN8rlQ-d"
      },
      "source": [
        "## Analyzed for 4 different values of $ \\ Alpha $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqXQRgg5lPq7"
      },
      "source": [
        "\n",
        "def sarsa():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times1 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times1  += oneEpisode_s(Q)\n",
        "        x_sarsa_Alpha.append(ep)\n",
        "        y_sarsa_Alpha.append(total_times1/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    \n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if ALPHA == ALPHAS[0]:\n",
        "        plt.plot(x_sarsa_Alpha, y_sarsa_Alpha, color='forestgreen', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[1]:\n",
        "        plt.plot(x_sarsa_Alpha, y_sarsa_Alpha, color='royalblue', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[2]:\n",
        "        plt.plot(x_sarsa_Alpha, y_sarsa_Alpha, color='red', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[3]:\n",
        "        plt.plot(x_sarsa_Alpha, y_sarsa_Alpha, color='purple', label='Alpha='+str(ALPHA))\n",
        "\n",
        "\n",
        "def q_learning():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times2 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times2  += oneEpisode_q(Q)\n",
        "        x_q_Alpha.append(ep)\n",
        "        y_q_Alpha.append(total_times2/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if ALPHA == ALPHAS[0]:\n",
        "        plt.plot(x_q_Alpha, y_q_Alpha, color='forestgreen', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[1]:\n",
        "        plt.plot(x_q_Alpha, y_q_Alpha, color='royalblue', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[2]:\n",
        "        plt.plot(x_q_Alpha, y_q_Alpha, color='red', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[3]:\n",
        "        plt.plot(x_q_Alpha, y_q_Alpha, color='purple', label='Alpha='+str(ALPHA))\n",
        "\n",
        "\n",
        "def sarsa_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times3 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times3  += oneEpisode_S_lambda(Q)\n",
        "        x_sarsaL_Alpha.append(ep)\n",
        "        y_sarsaL_Alpha.append(total_times3/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if ALPHA == ALPHAS[0]:\n",
        "        plt.plot(x_sarsaL_Alpha, y_sarsaL_Alpha, color='forestgreen', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[1]:\n",
        "        plt.plot(x_sarsaL_Alpha, y_sarsaL_Alpha, color='royalblue', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[2]:\n",
        "        plt.plot(x_sarsaL_Alpha, y_sarsaL_Alpha, color='red', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[3]:\n",
        "        plt.plot(x_sarsaL_Alpha, y_sarsaL_Alpha, color='purple', label='Alpha='+str(ALPHA))\n",
        "\n",
        "\n",
        "def q_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times4 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times4  += oneEpisode_q_Lambda(Q)\n",
        "        x_ql_Alpha.append(ep)\n",
        "        y_ql_Alpha.append(total_times4/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if ALPHA == ALPHAS[0]:\n",
        "        plt.plot(x_ql_Alpha, y_ql_Alpha, color='forestgreen', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[1]:\n",
        "        plt.plot(x_ql_Alpha, y_ql_Alpha, color='royalblue', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[2]:\n",
        "        plt.plot(x_ql_Alpha, y_ql_Alpha, color='red', label='Alpha='+str(ALPHA))\n",
        "    if ALPHA == ALPHAS[3]:\n",
        "        plt.plot(x_ql_Alpha, y_ql_Alpha, color='purple', label='Alpha='+str(ALPHA))\n",
        "\n",
        "   \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Learning rate or step size\n",
        "    ALPHAS = [0.1,0.2,0.4,0.5]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for ALPHA in ALPHAS:\n",
        "       print('Calculating convergence for ALPHA in Sarsa= ', ALPHA)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_sarsa_Alpha = []\n",
        "       y_sarsa_Alpha = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       sarsa()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $ALPHA$ in Sarsa')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show() \n",
        "##########################\n",
        "  \n",
        "    # Learning rate or step size\n",
        "    ALPHAS = [0.1,0.2,0.4,0.5]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for ALPHA in ALPHAS:\n",
        "       print('Calculating convergence for ALPHA in Q-Learning= ', ALPHA)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_q_Alpha = []\n",
        "       y_q_Alpha = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       q_learning()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $ALPHA$ in Q-Learning')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()\n",
        "\n",
        "###########################\n",
        "\n",
        "    # Learning rate or step size\n",
        "    ALPHAS = [0.1,0.2,0.4,0.5]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for ALPHA in ALPHAS:\n",
        "       print('Calculating convergence for ALPHA in Sarsa-Lambda= ', ALPHA)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_sarsaL_Alpha = []\n",
        "       y_sarsaL_Alpha = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       sarsa_Lambda()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $ALPHA$ in Sarsa-Lambda')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()\n",
        "\n",
        "###########################\n",
        "\n",
        "    # Learning rate or step size\n",
        "    ALPHAS =[0.1,0.2,0.4,0.5]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for ALPHA in ALPHAS:\n",
        "       print('Calculating convergence for ALPHA in Q-Lambda= ', ALPHA)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_ql_Alpha = []\n",
        "       y_ql_Alpha = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       q_Lambda()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $ALPHA$ in Q-Lambda')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "306MD83JlTF6"
      },
      "source": [
        " ## Analyzed for 4 different values of $ \\ Epsilon $."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6nB0ECkr5YK"
      },
      "source": [
        "\n",
        "def sarsa():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times1 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times1  += oneEpisode_s(Q)\n",
        "        x_sarsa_EPSILON.append(ep)\n",
        "        y_sarsa_EPSILON.append(total_times1/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    \n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if EPSILON == EPSILONs[0]:\n",
        "        plt.plot(x_sarsa_EPSILON, y_sarsa_EPSILON, color='forestgreen', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[1]:\n",
        "        plt.plot(x_sarsa_EPSILON, y_sarsa_EPSILON, color='royalblue', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[2]:\n",
        "        plt.plot(x_sarsa_EPSILON, y_sarsa_EPSILON, color='red', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[3]:\n",
        "        plt.plot(x_sarsa_EPSILON, y_sarsa_EPSILON, color='purple', label='EPSILON='+str(EPSILON))\n",
        "\n",
        "\n",
        "def q_learning():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times2 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times2  += oneEpisode_q(Q)\n",
        "        x_q_EPSILON.append(ep)\n",
        "        y_q_EPSILON.append(total_times2/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if EPSILON == EPSILONs[0]:\n",
        "        plt.plot(x_q_EPSILON, y_q_EPSILON, color='forestgreen', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[1]:\n",
        "        plt.plot(x_q_EPSILON, y_q_EPSILON, color='royalblue', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[2]:\n",
        "        plt.plot(x_q_EPSILON, y_q_EPSILON, color='red', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[3]:\n",
        "        plt.plot(x_q_EPSILON, y_q_EPSILON, color='purple', label='EPSILON='+str(EPSILON))\n",
        "\n",
        "\n",
        "def sarsa_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times3 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times3  += oneEpisode_S_lambda(Q)\n",
        "        x_sarsaL_EPSILON.append(ep)\n",
        "        y_sarsaL_EPSILON.append(total_times3/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if EPSILON == EPSILONs[0]:\n",
        "        plt.plot(x_sarsaL_EPSILON, y_sarsaL_EPSILON, color='forestgreen', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[1]:\n",
        "        plt.plot(x_sarsaL_EPSILON, y_sarsaL_EPSILON, color='royalblue', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[2]:\n",
        "        plt.plot(x_sarsaL_EPSILON, y_sarsaL_EPSILON, color='red', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[3]:\n",
        "        plt.plot(x_sarsaL_EPSILON, y_sarsaL_EPSILON, color='purple', label='EPSILON='+str(EPSILON))\n",
        "\n",
        "\n",
        "def q_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    total_times4 = 0\n",
        "\n",
        "    while ep < episodeLimit:\n",
        "\n",
        "        total_times4  += oneEpisode_q_Lambda(Q)\n",
        "        x_ql_EPSILON.append(ep)\n",
        "        y_ql_EPSILON.append(total_times4/(ep+1))\n",
        "        ep = ep+1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "        optimalPolicy.append([])\n",
        "        for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "\n",
        "    if EPSILON == EPSILONs[0]:\n",
        "        plt.plot(x_ql_EPSILON, y_ql_EPSILON, color='forestgreen', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[1]:\n",
        "        plt.plot(x_ql_EPSILON, y_ql_EPSILON, color='royalblue', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[2]:\n",
        "        plt.plot(x_ql_EPSILON, y_ql_EPSILON, color='red', label='EPSILON='+str(EPSILON))\n",
        "    if EPSILON == EPSILONs[3]:\n",
        "        plt.plot(x_ql_EPSILON, y_ql_EPSILON, color='purple', label='EPSILON='+str(EPSILON))\n",
        "\n",
        "   \n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Learning rate or step size\n",
        "    EPSILONs = [0.01,0.1,0.2,0.3]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for EPSILON in EPSILONs:\n",
        "       print('Calculating convergence for EPSILON in Sarsa= ', EPSILON)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_sarsa_EPSILON = []\n",
        "       y_sarsa_EPSILON = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       sarsa()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $EPSILON$ in Sarsa')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show() \n",
        "##########################\n",
        "  \n",
        "    # Learning rate or step size\n",
        "    EPSILONs = [0.01,0.1,0.2,0.3]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for EPSILON in EPSILONs:\n",
        "       print('Calculating convergence for EPSILON in Q-Learning= ', EPSILON)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_q_EPSILON = []\n",
        "       y_q_EPSILON = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       q_learning()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $EPSILON$ in Q-Learning')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()\n",
        "\n",
        "###########################\n",
        "\n",
        "    # Learning rate or step size\n",
        "    EPSILONs = [0.01,0.1,0.2,0.3]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for EPSILON in EPSILONs:\n",
        "       print('Calculating convergence for EPSILON in Sarsa-Lambda= ', EPSILON)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_sarsaL_EPSILON = []\n",
        "       y_sarsaL_EPSILON = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       sarsa_Lambda()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $EPSILON$ in Sarsa-Lambda')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()\n",
        "\n",
        "###########################\n",
        "\n",
        "    # Learning rate or step size\n",
        "    EPSILONs = [0.01,0.1,0.2,0.3]\n",
        "    plt.figure(figsize=(10,8))\n",
        "\n",
        "    for EPSILON in EPSILONs:\n",
        "       print('Calculating convergence for EPSILON in Q-Lambda= ', EPSILON)\n",
        "\n",
        "       # Eligibility trace \n",
        "       e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "    \n",
        "    \n",
        "       x_ql_EPSILON = []\n",
        "       y_ql_EPSILON = [] \n",
        "    \n",
        "    \n",
        "       Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
        "\n",
        "       START = [3, 0]\n",
        "       GOAL = [3, 7]\n",
        "    \n",
        "       actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
        "       q_Lambda()\n",
        "\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Analyzed for 4 different values of $EPSILON$ in Q-Lambda')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average time steps')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE0e6Kp2sJ-_"
      },
      "source": [
        "# King's Moves + Stochastic Wind\n",
        "The implementation with using the King's Moves plus Stochastic Wind . Re-solve the windy gridworld task assuming eight possible actions, including the diagonal moves, rather than the usual four. Assuming that the effect of the wind, if there is any, is stochastic, sometimes varying by 1 from the mean values given for each column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aP2t0UUsInh"
      },
      "source": [
        "# world height\n",
        "WORLD_HEIGHT = 7\n",
        "\n",
        "# world width\n",
        "WORLD_WIDTH = 10\n",
        "\n",
        "# probability for exploration\n",
        "EPSILON = 0.2\n",
        "\n",
        "# Learning rate or step size\n",
        "ALPHA = 0.2\n",
        "\n",
        "# reward\n",
        "REWARD = -1.0\n",
        "\n",
        "# GAMMA\n",
        "GAMMA = 0.75\n",
        "\n",
        "# Lambda\n",
        "LAMBDA = 0.5\n",
        "\n",
        "# wind strength for each column\n",
        "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "\n",
        "# possible actions\n",
        "ACTION_UP = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_LEFT = 2\n",
        "ACTION_RIGHT = 3\n",
        "ACTION_UP_LEFT = 4\n",
        "ACTION_UP_RIGHT = 5\n",
        "ACTION_DOWN_RIGHT = 6\n",
        "ACTION_DOWN_LEFT = 7\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "\n",
        "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, \n",
        "           ACTION_UP_LEFT, ACTION_UP_RIGHT, ACTION_DOWN_RIGHT, ACTION_DOWN_LEFT]\n",
        "\n",
        "\n",
        "START = [3, 0]\n",
        "GOAL = [3, 7]\n",
        "\n",
        "# eligibility trace\n",
        "e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "\n",
        "# STOCHASTIC\n",
        "# stochastic = 1\n",
        "###############################  Functions  ####################################    \n",
        "\n",
        "# state transitions\n",
        "def step(currentState, currentAction):\n",
        "\n",
        "    i, j = currentState\n",
        "\n",
        "    # Because of stochastic issue, actions are modified accordingly \n",
        "    if WIND[j] != 0:\n",
        "        # Up\n",
        "      if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - WIND[j], 0), j]\n",
        "      #Down\n",
        "      elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), j]\n",
        "      # Left\n",
        "      elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - WIND[j], 0), max(j - 1, 0)]\n",
        "      # Right\n",
        "      elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Up-Left\n",
        "      elif currentAction == ACTION_UP_LEFT:\n",
        "        return [max(i - 1 - WIND[j], 0), max(j - 1, 0)]\n",
        "      # Up-Right\n",
        "      elif currentAction == ACTION_UP_RIGHT:\n",
        "        return [max(i - 1 - WIND[j], 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Down-Right\n",
        "      elif currentAction == ACTION_DOWN_RIGHT:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Down-Left\n",
        "      elif currentAction == ACTION_DOWN_LEFT:\n",
        "        return [max(min(i + 1 - WIND[j], WORLD_HEIGHT - 1), 0), max(j - 1, 0)]\n",
        "\n",
        "      else:\n",
        "        assert False\n",
        "\n",
        "    else:\n",
        "    \n",
        "      wind_strength = WIND[int(currentState[0])] + random.randint(-1, 1)\n",
        "\n",
        "      # Up\n",
        "      if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - wind_strength, 0), j]\n",
        "      #Down\n",
        "      elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), j]\n",
        "      # Left\n",
        "      elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - wind_strength, 0), max(j - 1, 0)]\n",
        "      # Right\n",
        "      elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - wind_strength, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Up-Left\n",
        "      elif currentAction == ACTION_UP_LEFT:\n",
        "        return [max(i - 1 - wind_strength, 0), max(j - 1, 0)]\n",
        "      # Up-Right\n",
        "      elif currentAction == ACTION_UP_RIGHT:\n",
        "        return [max(i - 1 - wind_strength, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Down-Right\n",
        "      elif currentAction == ACTION_DOWN_RIGHT:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "      # Down-Left\n",
        "      elif currentAction == ACTION_DOWN_LEFT:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), max(j - 1, 0)]\n",
        "\n",
        "      else:\n",
        "        assert False\n",
        "\n",
        "\t\n",
        "\n",
        "\n",
        "# interaction in a Sarsa episode\n",
        "def oneEpisode_s(Q):\n",
        "  \n",
        "    # track the total time steps in this episode\n",
        "    time = 0\n",
        "\n",
        "    # initialize state\n",
        "    currentState = START\n",
        "\n",
        "    # choose an action based on epsilon-greedy algorithm\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:   \n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "                            \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "# interaction in a Sarsa_Lambda episode\n",
        "def oneEpisode_S_lambda2(Q):\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    currentState = START\n",
        "\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        newState = step(currentState, currentAction)\n",
        "\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Sarsa lambda update\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], newAction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    e[i ,j, action] *= LAMBDA * GAMMA\n",
        "\n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-learning episode\n",
        "def oneEpisode_q(Q):\n",
        "    \n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "    \n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        # selecting the new action among all possible action which gives the MAX Q\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Q-learning update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD +  GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "\n",
        "        currentState = newState\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Watkins‚Äôs Q(Œª) episode\n",
        "def oneEpisode_q_Lambda2(Q):\n",
        "\n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Watkins‚Äôs Q_Lambda update          \n",
        "        Bestaction = np.argmax(Q[newState[0], newState[1], actions])\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], Bestaction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    if newAction == Bestaction:\n",
        "                          e[i ,j, action] *= LAMBDA * GAMMA\n",
        "                    else:\n",
        "                          e[i ,j, action] == 0      \n",
        "  \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "def sarsa():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    #stochastic == 1\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_s(Q))\n",
        "            time = oneEpisode_s(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='royalblue')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 8 action SARSA, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "\n",
        "\n",
        "    \n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\t    Optimal policy SARSA is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "\n",
        "def sarsa_Lambda2():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    #stochastic == 1\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_S_lambda2(Q))\n",
        "            time = oneEpisode_S_lambda2(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='red')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 8 action SARSA ($\\lambda$), alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\tOptimal policy SARSA_lambda is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "def q_learning():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q(Q))\n",
        "            time = oneEpisode_q(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='forestgreen')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 8 action Q-learning, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\t Optimal policy Q-learning is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "\n",
        "\n",
        "def q_Lambda():\n",
        "\n",
        "    Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "    # number of episodes\n",
        "    episodeLimit = 1000\n",
        "    ep = 0\n",
        "    episodes = []\n",
        "    while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q_Lambda2(Q))\n",
        "            time = oneEpisode_q_Lambda2(Q)\n",
        "            episodes.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "    #episodes = np.add.accumulate(episodes)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(episodes, color='purple')\n",
        "    plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "    plt.title(\"Windy Gridworld 8 action Watkins‚Äôs Q ($\\lambda$), alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "    plt.xlabel('Time steps')\n",
        "    plt.ylabel('Episodes')\n",
        "    plt.show()\n",
        "\n",
        "    # display the optimal policy\n",
        "    optimalPolicy = []\n",
        "    for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "    print('\\n-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-') \n",
        "    print('\\tOptimal policy Watkins‚Äôs Q(Œª) is:    \\n')         \n",
        "    for row in optimalPolicy:\n",
        "            print(row)\n",
        "    print('\\n{}'.format([str(w) for w in WIND]))\n",
        "    print('\\t Wind strength for each column\\n')\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sarsa()\n",
        "    sarsa_Lambda2()\n",
        "    q_Lambda()\n",
        "    q_learning()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQbPIZ855-zI"
      },
      "source": [
        "# Compare all algorithms in 8 action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uXjLEe75oP5"
      },
      "source": [
        "\n",
        "#######################   Comparing   ######################\n",
        "\n",
        "WORLD_HEIGHT = 7\n",
        "WORLD_WIDTH = 10\n",
        "EPSILON = 0.01\n",
        "ALPHA = 0.1\n",
        "REWARD = -1.0\n",
        "GAMMA = 0.9\n",
        "LAMBDA = 0.8\n",
        "\n",
        "# wind \n",
        "WIND = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "\n",
        "ACTION_UP = 0\n",
        "ACTION_DOWN = 1\n",
        "ACTION_LEFT = 2\n",
        "ACTION_RIGHT = 3\n",
        "ACTION_UP_LEFT = 4\n",
        "ACTION_UP_RIGHT = 5\n",
        "ACTION_DOWN_RIGHT = 6\n",
        "ACTION_DOWN_LEFT = 7\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "\n",
        "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, \n",
        "           ACTION_UP_LEFT, ACTION_UP_RIGHT, ACTION_DOWN_RIGHT, ACTION_DOWN_LEFT]\n",
        "\n",
        "START = [3, 0]\n",
        "GOAL = [3, 7]\n",
        "\n",
        "# eligibility trace\n",
        "e = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "\n",
        "###############################  Functions  ####################################    \n",
        "\n",
        "# state transitions\n",
        "def step(currentState, currentAction):\n",
        "\n",
        "    i, j = currentState\n",
        "\n",
        "    # Because of stochastic issue, actions are modified accordingly\n",
        "    wind_strength = WIND[int(currentState[0])] + random.randint(-1, 1)\n",
        "\n",
        "    # Up\n",
        "    if currentAction == ACTION_UP:\n",
        "        return [max(i - 1 - wind_strength, 0), j]\n",
        "    #Down\n",
        "    elif currentAction == ACTION_DOWN:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), j]\n",
        "    # Left\n",
        "    elif currentAction == ACTION_LEFT:\n",
        "        return [max(i - wind_strength, 0), max(j - 1, 0)]\n",
        "    # Right\n",
        "    elif currentAction == ACTION_RIGHT:\n",
        "        return [max(i - wind_strength, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    # Up-Left\n",
        "    elif currentAction == ACTION_UP_LEFT:\n",
        "        return [max(i - 1 - wind_strength, 0), max(j - 1, 0)]\n",
        "    # Up-Right\n",
        "    elif currentAction == ACTION_UP_RIGHT:\n",
        "        return [max(i - 1 - wind_strength, 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    # Down-Right\n",
        "    elif currentAction == ACTION_DOWN_RIGHT:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), min(j + 1, WORLD_WIDTH - 1)]\n",
        "    # Down-Left\n",
        "    elif currentAction == ACTION_DOWN_LEFT:\n",
        "        return [max(min(i + 1 - wind_strength, WORLD_HEIGHT - 1), 0), max(j - 1, 0)]\n",
        "\n",
        "    else:\n",
        "        assert False\n",
        "\n",
        "\n",
        "\n",
        "# interaction in a Sarsa episode\n",
        "def oneEpisode_s(Q):\n",
        "  \n",
        "    # track the total time steps in this episode\n",
        "    time = 0\n",
        "\n",
        "    # initialize state\n",
        "    currentState = START\n",
        "\n",
        "    # choose an action based on epsilon-greedy algorithm\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:   \n",
        "\n",
        "        newState = step(currentState, currentAction)\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Sarsa update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD + Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "                            \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "# interaction in a Sarsa_Lambda episode\n",
        "def oneEpisode_S_lambda2(Q):\n",
        "\n",
        "    time = 0\n",
        "\n",
        "    currentState = START\n",
        "\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "    else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])\n",
        "\n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        newState = step(currentState, currentAction)\n",
        "\n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            newAction = np.random.choice(actions)\n",
        "        else:\n",
        "            newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Sarsa lambda update\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], newAction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    e[i ,j, action] *= LAMBDA * GAMMA\n",
        "\n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Q-learning episode\n",
        "def oneEpisode_q(Q):\n",
        "    \n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "    \n",
        "    # keep going until get to the goal state\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        # selecting the new action among all possible action which gives the MAX Q\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "        \n",
        "        # Q-learning update\n",
        "        Q[currentState[0], currentState[1], currentAction] += \\\n",
        "                            ALPHA * (REWARD +  GAMMA * Q[newState[0], newState[1], newAction] -\n",
        "                            Q[currentState[0], currentState[1], currentAction])\n",
        "\n",
        "        currentState = newState\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "# interaction in a Watkins‚Äôs Q(Œª) episode\n",
        "def oneEpisode_q_Lambda2(Q):\n",
        "\n",
        "    time = 0\n",
        "    \n",
        "    currentState = START\n",
        "\n",
        "    while currentState != GOAL:\n",
        "        \n",
        "        \n",
        "        if np.random.binomial(1, EPSILON) == 1:\n",
        "            currentAction = np.random.choice(actions)\n",
        "        else:\n",
        "            currentAction = np.argmax(Q[currentState[0], currentState[1], :])  \n",
        "        newState = step(currentState, currentAction)\n",
        "        newAction = np.argmax(Q[newState[0], newState[1], :])\n",
        "\n",
        "        # Watkins‚Äôs Q_Lambda update          \n",
        "        Bestaction = np.argmax(Q[newState[0], newState[1], actions])\n",
        "\n",
        "        DELTA = REWARD + GAMMA * Q[newState[0], newState[1], Bestaction] - Q[currentState[0], currentState[1], currentAction]\n",
        "        e[currentState[0], currentState[1], currentAction] += 1\n",
        "        for i in range(0, WORLD_HEIGHT):\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                for action in actions:\n",
        "                    Q[i, j, action] += ALPHA * DELTA * e[i, j, action]\n",
        "                    if newAction == Bestaction:\n",
        "                          e[i ,j, action] *= LAMBDA * GAMMA\n",
        "                    else:\n",
        "                          e[i ,j, action] == 0      \n",
        "  \n",
        "        currentState = newState\n",
        "        currentAction = newAction\n",
        "\n",
        "        time += 1\n",
        "\n",
        "    return time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes1 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_s(Q))\n",
        "            time = oneEpisode_s(Q)\n",
        "            episodes1.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes2 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q(Q))\n",
        "            time = oneEpisode_q(Q)\n",
        "            episodes2.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes3 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_S_lambda(Q))\n",
        "            time = oneEpisode_S_lambda2(Q)\n",
        "            episodes3.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Q = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 8))\n",
        "# number of episodes\n",
        "episodeLimit = 1000\n",
        "ep = 0\n",
        "episodes4 = []\n",
        "while ep < episodeLimit:\n",
        "            #episodes.append(oneEpisode_q_Lambda(Q))\n",
        "            time = oneEpisode_q_Lambda2(Q)\n",
        "            episodes4.extend([ep] * time)\n",
        "            ep += 1\n",
        "\n",
        "\n",
        "\n",
        "# display the optimal policy\n",
        "optimalPolicy = []\n",
        "for i in range(0, WORLD_HEIGHT):\n",
        "            optimalPolicy.append([])\n",
        "            for j in range(0, WORLD_WIDTH):\n",
        "                if [i, j] == GOAL:\n",
        "                     optimalPolicy[-1].append('G')\n",
        "                     continue\n",
        "                bestAction = np.argmax(Q[i, j, :])\n",
        "                #print(Q)\n",
        "                if bestAction == ACTION_UP:\n",
        "                     optimalPolicy[-1].append('‚Üë')\n",
        "                elif bestAction == ACTION_DOWN:\n",
        "                     optimalPolicy[-1].append('‚Üì')\n",
        "                elif bestAction == ACTION_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üê')\n",
        "                elif bestAction == ACTION_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üí')\n",
        "                elif bestAction == ACTION_UP_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üñ')\n",
        "                elif bestAction == ACTION_UP_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üó')\n",
        "                elif bestAction == ACTION_DOWN_RIGHT:\n",
        "                     optimalPolicy[-1].append('‚Üò')\n",
        "                elif bestAction == ACTION_DOWN_LEFT:\n",
        "                     optimalPolicy[-1].append('‚Üô') \n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(episodes1, color='darkblue',label=\"SARSA\")\n",
        "plt.plot(episodes3, color='seagreen',label=\"SARSA(Œª)\")\n",
        "plt.plot(episodes2, color='red',label=\"Q Learning\")\n",
        "plt.plot(episodes4, color='gold',label=\"Q(Œª)\")\n",
        "plt.grid(b=True, which='major', color='#666666', linestyle='--')\n",
        "plt.title(\"All (Windy Gridworld) algorithms - 8 action, alpha={}, epsilon={}\".format(ALPHA, EPSILON))\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('Time steps')\n",
        "plt.ylabel('Episodes')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}